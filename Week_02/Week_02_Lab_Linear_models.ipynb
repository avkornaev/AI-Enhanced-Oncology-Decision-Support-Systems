{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmmEFmHDYy0uL8EyTawnKE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avkornaev/AI-Enhanced-Oncology-Decision-Support-Systems/blob/main/Week_02/Week_02_Lab_Linear_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hands-on Session: Word Embeddings\n",
        "This notebook was developed using methodologies suggested by ChatGPT (OpenAI, 2025)"
      ],
      "metadata": {
        "id": "AOpKnoaRdaK-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install and import libraries"
      ],
      "metadata": {
        "id": "e1F8TJNNsNhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim nltk matplotlib\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# For pretrained vectors\n",
        "import gensim.downloader as api\n",
        "\n",
        "nltk.download(\"punkt\")       # basic sentence tokenizer\n",
        "nltk.download(\"punkt_tab\")   # (newer NLTK versions require this too)"
      ],
      "metadata": {
        "id": "Tx4OEwWrsN09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PART 1: Tiny Toy Corpus"
      ],
      "metadata": {
        "id": "ZJWLSZsjsfqI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== PART 1: Tiny toy corpus ===\")\n",
        "\n",
        "# 1. Define a tiny \"made-up\" dataset\n",
        "toy_sentences = [\n",
        "    [\"king\", \"queen\", \"man\", \"woman\"],\n",
        "    [\"robot\", \"computer\", \"machine\", \"automation\"],\n",
        "    [\"cat\", \"dog\", \"animal\", \"pet\"],\n",
        "    [\"car\", \"bus\", \"train\", \"transport\"],\n",
        "    [\"doctor\", \"nurse\", \"hospital\", \"medicine\"],\n",
        "    [\"teacher\", \"student\", \"school\", \"education\"],\n",
        "    [\"apple\", \"banana\", \"fruit\", \"food\"]\n",
        "]\n",
        "\n",
        "# 2. Train Word2Vec on this toy corpus\n",
        "toy_model = Word2Vec(toy_sentences, vector_size=20, window=2, min_count=1, sg=1, epochs=500)\n",
        "\n",
        "# 3. Show some vectors\n",
        "print(\"\\nVector for 'king':\\n\", toy_model.wv[\"king\"], \"\\n\")\n",
        "\n",
        "# 4. Check similarities\n",
        "print(\"Similarity king-queen:\", toy_model.wv.similarity(\"king\", \"queen\"))\n",
        "print(\"Similarity king-robot:\", toy_model.wv.similarity(\"king\", \"robot\"))\n",
        "\n",
        "# 5. Analogy test\n",
        "print(\"\\nAnalogy: king - man + woman ≈ ?\")\n",
        "print(toy_model.wv.most_similar(positive=[\"king\",\"woman\"], negative=[\"man\"], topn=3))\n",
        "\n",
        "# 6. Visualization of toy embeddings\n",
        "words = list(toy_model.wv.key_to_index)\n",
        "X = toy_model.wv[words]\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "result = pca.fit_transform(X)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(result[:, 0], result[:, 1])\n",
        "for i, word in enumerate(words):\n",
        "    plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
        "plt.title(\"Toy Word Embeddings (PCA projection)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SCjn25pKsgTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PART 2: Real Corpus (Shakespeare Macbeth)"
      ],
      "metadata": {
        "id": "an0ARfeYsp0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== PART 2: Shakespeare Macbeth ===\")\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"gutenberg\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "\n",
        "from nltk.corpus import gutenberg\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download(\"gutenberg\")\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "# Load Hamlet\n",
        "raw_text = gutenberg.raw(\"shakespeare-hamlet.txt\")\n",
        "\n",
        "# Preprocess: lowercase and remove punctuation\n",
        "sentences = []\n",
        "for sent in sent_tokenize(raw_text):\n",
        "    tokens = [w.lower() for w in word_tokenize(sent) if w.isalpha()]\n",
        "    if tokens:\n",
        "        sentences.append(tokens)\n",
        "\n",
        "# Train Word2Vec\n",
        "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4, sg=1, epochs=20)\n",
        "\n",
        "# Vocabulary size\n",
        "print(\"Vocabulary size:\", len(model.wv))\n",
        "\n",
        "# Similarities\n",
        "def safe_sim(word1, word2):\n",
        "    try:\n",
        "        return model.wv.similarity(word1, word2)\n",
        "    except KeyError:\n",
        "        return f\"{word1} or {word2} not in vocab\"\n",
        "\n",
        "print(\"\\nSimilarity king-queen:\", safe_sim(\"king\",\"queen\"))\n",
        "print(\"Similarity king-man:\", safe_sim(\"king\",\"man\"))\n",
        "print(\"Similarity queen-woman:\", safe_sim(\"queen\",\"woman\"))\n",
        "\n",
        "# Analogy test\n",
        "try:\n",
        "    result = model.wv.most_similar(positive=[\"king\",\"woman\"], negative=[\"man\"], topn=3)\n",
        "    print(\"\\nAnalogy: king - man + woman ≈\", result)\n",
        "except KeyError as e:\n",
        "    print(\"Analogy failed:\", e)\n",
        "\n",
        "# Visualization of selected words\n",
        "target_words = [\"king\",\"queen\",\"man\",\"woman\",\"hamlet\",\"claudius\",\"ophelia\",\"polonius\"]\n",
        "words_in_vocab = [w for w in target_words if w in model.wv]\n",
        "X = model.wv[words_in_vocab]\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "result = pca.fit_transform(X)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(result[:, 0], result[:, 1])\n",
        "for i, word in enumerate(words_in_vocab):\n",
        "    plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
        "plt.title(\"Hamlet Word Embeddings (PCA projection)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZLz3HeoGshaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PART 3: Pretrained Embeddings"
      ],
      "metadata": {
        "id": "VmBa3fzLs0Wa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== PART 3: Pretrained GloVe vectors ===\")\n",
        "\n",
        "# Load pretrained GloVe (50-dim, trained on Wikipedia + Gigaword)\n",
        "glove_model = api.load(\"glove-wiki-gigaword-50\")\n",
        "\n",
        "# Real semantic similarities\n",
        "print(\"\\nSimilarity king-queen:\", glove_model.similarity(\"king\", \"queen\"))\n",
        "print(\"Similarity king-robot:\", glove_model.similarity(\"king\", \"robot\"))\n",
        "\n",
        "# Analogy test\n",
        "print(\"\\nAnalogy: king - man + woman ≈ ?\")\n",
        "print(glove_model.most_similar(positive=[\"king\",\"woman\"], negative=[\"man\"], topn=5))\n",
        "\n",
        "# Visualization of selected embeddings\n",
        "target_words = [\"king\",\"queen\",\"man\",\"woman\",\"prince\",\"princess\",\"doctor\",\"nurse\",\"robot\",\"machine\"]\n",
        "X = glove_model[target_words]\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "result = pca.fit_transform(X)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(result[:, 0], result[:, 1])\n",
        "for i, word in enumerate(target_words):\n",
        "    plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
        "plt.title(\"Pretrained GloVe Word Embeddings (PCA projection)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "a6dBxYMbshXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STUDENT ACTIVITIES"
      ],
      "metadata": {
        "id": "WmKZEaLZtK7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== Student Exercises ===\")\n",
        "print(\"\"\"\n",
        "1. Modify the toy corpus in PART 1 by adding your own sentences (e.g., robotics, vision, AI).\n",
        "   - Observe how adding new data changes similarities and analogies.\n",
        "\n",
        "2. Change training parameters:\n",
        "   - Try vector_size=10 vs 100, window=2 vs 10, sg=0 (CBOW) vs sg=1 (Skip-gram).\n",
        "   - Compare results.\n",
        "\n",
        "3. In PART 2, explore Shakespeare embeddings:\n",
        "   - Check similarities between words like \"king\", \"death\", \"love\".\n",
        "   - Visualize different sets of words.\n",
        "\n",
        "4. In PART 3, query pretrained embeddings:\n",
        "   - Try analogies: \"paris - france + italy ≈ ?\", \"camera - image + sound ≈ ?\".\n",
        "   - Pick robotics/CV words and test them.\n",
        "\n",
        "Discussion:\n",
        "- Why do toy embeddings fail? (data scale)\n",
        "- Why do pretrained embeddings capture meaning better?\n",
        "- How do LLMs build on embeddings? (contextual, dynamic representations)\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "P4EYxmRkfT1e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}